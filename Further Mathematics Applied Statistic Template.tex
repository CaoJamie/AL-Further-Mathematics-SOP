\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\date{}

\begin{document}

\section{Further Mathematics Statistics Template}\label{header-n0}

\subsection{Distributions}\label{header-n2}

\subsubsection{Combining Variables}\label{header-n3}

For two separate variables \(X\) and \(Y\), we could have Linear
combination \(Z = aX + bY\)

Hence that we can obtain:

\begin{itemize}
\item
  \(E(Z) = E(aX + bY) = aE(X) + bE(Y)\)
\item
  \(Var(Z) = a^2Var(X) + b^2Var(Y)\)
\end{itemize}

Therefore, if we combined two variable
\(X \sim \mathcal{N}(\bar{x},\sigma_x^2)\) and
\(Y \sim \mathcal{N}(\bar{y},\sigma_y^2)\) as \(Z = aX + bY\), we would
have that
\(Z \sim \mathcal{N}(a\bar{x} + b\bar{y},a^2\sigma_x^2 + b^2\sigma_y^2)\)

Similarly, if we combined two variable \(X\sim Po(\lambda_x)\) and
\(Y\sim Po(\lambda_y)\) as \(Z = aX + bY\), we would have that
\(Z \sim Po(a\lambda_x+b\lambda_y)\)

Notice that \(aX\) is not the same as \(\underbrace{X+X+ \dots + X}_a\),
the lateral one means all of the variables are independent of each
other.

\subsubsection{PDF and CDF}\label{header-n14}

\paragraph{Relationship between PDF and CDF}\label{header-n15}

a PDF describing the probability for a distribution \(X\) can be
expressed as:

\(f(x) = \begin{cases}p(x) &a<x<b\\q(x)&b<x<c\\\dots\\0  & otherwise\end{cases}\)

\(P(a<x<b) = \int^b_a f(x) dx\), note that individual \(P(X = a)\) for
PDF is always 0

Note that \(\int^{\infty}_{-\infty} f(x) dx  = 1\) and \(f(x) \ge 0\)

so that the CDF for the given PDF can be expressed as:

\(F(x) = \begin{cases}0&x<a\\P(x) = \int{p(x)} dx& a<x\le b\\Q(x) = \int {q(x)} dx & b<x\le c\\ \dots \\ 1& c<x\end{cases}\)

\(P(x<a) = F(a)\)

Note that \(F(x)\) is increase and that it reached 1 after threshold
value

(\(p(x),q(x),P(x),Q(x)\) are expression of x)

\paragraph{Calculating Statistics}\label{header-n25}

\begin{itemize}
\item
  \(E[g(x)] = \int^{\infty}_{-\infty} g(x)f(x)dx\)
\item
  \(Var[g(x)] = \int^{\infty}_{-\infty}g(x)^2f(x) dx - {E(g(x))}^2\)
\item
  Median \(x\) is
  that\(\int^x_{-\infty} f(u) du =\int^{\infty}_{x}fu) du =0.5\). also
  \(F(x) = 0.5\)
\item
  Mode \(x\) is the \(argmax_x f(x)\), which means that \(x\) has
  highest magnitude on \(f(x)\)
\end{itemize}

\paragraph{Changing Variable}\label{header-n35}

For a substitution of variable \(Y = p(X)\) for the CDF \(F(X)\), we
wished to obtain the correspondence CDF \(G(Y)\) , we want to consider
the approach by considering the definition of CDF

First we want to Obtain \(X = q(Y)\) given that \(Y = p(X)\)

Note that \(p[.]\) and \(q[.] \)all represent the relationship between
two variables

\(G(Y) = P(Y<y) = P(p(X)<y) = P(x<q(y)) = F(q(y))\)

Notice during this process some expression such as \(-x<y\) the
expression should be changed to \(1-P(x<X)\) and hence the CDF should
also be changed subsequently

hence you would be able to evaluate \(G(y)\)

the PDF \(g(y)\) could be obtain by taking \(\cfrac{d}{dx} G(y)\)

\subsubsection{Poisson distribution}\label{header-n43}

\(X\sim Po(\lambda)\) where \(\lambda\) is the mean number of event in
given interval

this is used to find

\paragraph{Condition for Poisson distribution}\label{header-n46}

\begin{itemize}
\item
  event occur randomly in space or time
\item
  event occur singly(cannot be simultaneously)
\item
  event occur independently
\item
  event occur at a constant rate
\end{itemize}

\paragraph{Probability}\label{header-n56}

\begin{itemize}
\item
  \( P(X = n) = \cfrac{e^{-\lambda} \lambda^n}{n!}\) for \(n \in \N\)
\item
  \(P(a \le X \le b) = \sum^b_{x = a}\cfrac{e^{-\lambda} \lambda^x}{x!}\)
\item
  \(P(X>n) = 1-P(x\le n) = 1-\sum^n_{x=0}\cfrac{e^{-\lambda} \lambda^n}{n!} \)
\end{itemize}

\paragraph{Means and Variance}\label{header-n64}

\begin{itemize}
\item
  \(E(x) = \lambda\)
\item
  \(Var(x) = \lambda\)
\end{itemize}

\paragraph{Approximate Poisson distribution with other
distribution}\label{header-n70}

\begin{figure}
\centering
\includegraphics[width=4.95833in]{1558161169977.png}
\caption{}\label{mermaid}
\end{figure}

\subparagraph{Continuity Correction}\label{header-n72}

when changing variable from discreate to continuous, continuity
correction is required to ensure the range cover all interval:

\begin{longtable}[]{@{}ll@{}}
\toprule
Discreate & Continuous\tabularnewline
\midrule
\endhead
\(X=a\) & \(a-0.5 \le X \le a+0.5\)\tabularnewline
\(X>a\) & \(X\ge a+0.5\)\tabularnewline
\(X \ge a\) & \(X \ge a-0.5\)\tabularnewline
\(X<a\) & \(X\le a-0.5\)\tabularnewline
\(X \le a\) & \(X \le a+0.5\)\tabularnewline
\bottomrule
\end{longtable}

\subparagraph{Binomial to Poisson}\label{header-n93}

if \(n>50\), \(np<5\) we could approximate binomial distribution using
Poisson distribution

Notice you should verify the condition before you conduct the
approximation

Specifically, we can have \(X \sim B(n,p) \rightarrow Y \sim Po(np)\)

\subparagraph{Poisson to Normal}\label{header-n97}

if \(\lambda > 15\), we could approximate Poisson distribution using
Normal distribution

Notice you should verify the condition before you conduct the
approximation

Specifically, we can have
\(X \sim Po(\lambda) \rightarrow Y \sim \mathcal{N}(\lambda,\lambda)\)

When obtaining the probability, you should use the continuity correction

\subparagraph{Binomial to Normal}\label{header-n102}

If \(np>5\) and \(nq > 5\) we could approximate binomial distribution
using Normal distribution

Notice you should verify the condition before you conduct the
approximation

Specifically, we can have
\(X \sim B(n,p) \rightarrow Y \sim \mathcal{N}(np, npq)\)

When obtaining the probability, you should use the continuity correction

\subsubsection{Geometric distribution}\label{header-n107}

\(X \sim Geo(p)\) where \(p\) is the probability of success for one
trail

this is used to find the number of attempt that leads to the first
success

\paragraph{Condition}\label{header-n110}

\begin{itemize}
\item
  Only two possible outcomes(success/failure)
\item
  Probability of success \(p\) is constant
\item
  Each event is independent
\end{itemize}

\paragraph{Probability}\label{header-n118}

\begin{itemize}
\item
  \(P(X = n) = (1-p)^{n-1}p\)
\item
  \(\begin{cases}P(X \le x) = 1-(1-p)^x \\ P(X > x) = (1-p)^x\end{cases}\)
\item
  \(\begin{cases}P(X < x) = 1-(1-p)^{x-1} \\ P(X \ge x) = (1-p)^{x-1}\end{cases}\)
\end{itemize}

\paragraph{Means and Variance}\label{header-n126}

\begin{itemize}
\item
  \(E(x) = \cfrac{1}{p}\)
\item
  \(Var(x) = \cfrac{q}{p^2}\)
\end{itemize}

\subsubsection{Negative exponential distribution}\label{header-n132}

\(X\sim Exp(\lambda)\) where \(\lambda\) is the average

this is used to model the duration of the event

\paragraph{Probability}\label{header-n135}

\begin{itemize}
\item
  \(P(a<X<b) = e^{-\lambda a} - e^{-\lambda b}\)
\item
  \(P(X<x) = 1-e^{-\lambda x}\)
\item
  \(P(X > x) = e^{-\lambda x}\)
\end{itemize}

\paragraph{Means and Variance}\label{header-n143}

\begin{itemize}
\item
  \(E(x) = \cfrac{1}{\lambda}\)
\item
  \(Var(x) =  \cfrac{1}{\lambda^2}\)
\end{itemize}

\paragraph{Memoryless}\label{header-n149}

This probability distribution have a very special property, which is
\textbf{memoryless}

This property suggest that the duration of the current event is
irrelevant to the previous event:

\(P(X>(a+b)|X>a) = P(X>b)\)

Please be aware of this property in the context of the question

\paragraph{PDF and CDF}\label{header-n154}

\begin{itemize}
\item
  \(f(x) = \begin{cases}\lambda e^{-\lambda x} & x \ge 0 \\ 0 & otherwise \end{cases}\)
  (Notice you can derive this from CDF)
\item
  \(F(x) = \begin{cases}1-e^{-\lambda x}&x\ge 0  \\ 0 & x<0 \end{cases}\)
  (Notice you should be able to obtain this through cumulative
  probability)
\end{itemize}

\subsection{Inference with Normal and t
distributions}\label{header-n160}

The decision Process on Choosing the type of Hypothesis test

\begin{figure}
\centering
\includegraphics[width=4.95833in]{1558161170072.png}
\caption{}\label{mermaid}
\end{figure}

if the variance \(\sigma^2\) is given, then it can used in the z test.
Otherwise, the unbiased estimator \(\hat{\sigma}^2\) should be
calculated. All numerical value involved in the test shall be calculated
before conducting the test:

\begin{itemize}
\item
  sample mean: \(\bar{x} = \cfrac{\sum x}{n_x}\)
\item
  unbiased sample variance:
  \(\hat{\sigma_x}^2 = \cfrac{n}{n-1}\bigg\{\cfrac{\sum x^2}{n} - \bar{x}^2\bigg\}\)
  (this is usually preferred in the FMA context for hypothesis test)
\end{itemize}

\subsubsection{One Sample Test}\label{header-n169}

\paragraph{Define the variable{[}if needed{]}}\label{header-n170}

\paragraph{Hypothesis}\label{header-n171}

\(H_0\): \(\mu_x = a\)

\(H_1\): \(\mu_x \ne a, \mu_x > a, \mu_x < a\)

\paragraph{Test Statistics}\label{header-n174}

Note that you should state CLT is used if only \(n>30\) is given

\(Z = \cfrac{\bar{x} - \mu_x}{\sqrt{\cfrac{\hat{\sigma}^2_x}{n_x}}}\) or
\(T = \cfrac{\bar{x} - \mu_x}{\sqrt{\cfrac{\hat{\sigma}^2_x}{n_x}}}\)

if it is a two tailed test, then obtain \(z_{\alpha/2}\) or
\(t_{\alpha/2 , n-1}\), else obtain \(z_{\alpha}\) or
\(t_{\alpha, n-1}\)(Note DGF = \(n-1\))

if \(|Z| < z\) or \(|T| < t\), \(H_0\) accepted, otherwise \(H_0\)
rejected

\paragraph{Conclusion}\label{header-n179}

Given the current data, there \{is/is not\} evidence to say that at
\(\alpha\)\% significant level, \{description of \(\mu_x\) hypothesis\}

\paragraph{Confidence Interval}\label{header-n181}

For Z test that the variance is known:
\([\bar{x} - z_{\alpha/2}\sqrt{\cfrac{\sigma^2_x}{n}} , \bar{x} + z_{\alpha/2}\sqrt{\cfrac{\sigma^2_x}{n}}]\)

For Z test that the variance is unknown:
\([\bar{x} - z_{\alpha/2}\sqrt{\cfrac{\hat{\sigma}^2_x}{n}},\bar{x} + z_{\alpha/2}\sqrt{\cfrac{\hat{\sigma}^2_x}{n}}]\)

For t test:
\([\bar{x} - t_{\alpha/2,n-1}\sqrt{\cfrac{\hat{\sigma}^2_x}{n}},\bar{x} + t_{\alpha/2,n-1}\sqrt{\cfrac{\hat{\sigma}^2_x}{n}}]\)

\subsubsection{Two sample test}\label{header-n185}

\paragraph{Define the variable{[}if needed{]}}\label{header-n186}

\paragraph{Hypothesis}\label{header-n187}

\(H_0\): \(\mu_x - \mu_y = a\)

\(H_1\): \(\mu_x - \mu_y \ne a /\mu_x - \mu_y> a /\mu_x - \mu_y < a\)

\paragraph{Test Statistics}\label{header-n190}

pooled estimate is required if \(n_1\), \(n_2\) \textless{} 30, but it
is always recommended unless question specified not to use

\(\hat{\sigma^2} = \cfrac{(n_x-1)\hat{\sigma}^2_x + (n_y-1)\hat{\sigma}^2_y}{n_x + n_y -2}\)

Note this formula used unbiased estimator of variance, the MF10 one is
the one with biased estimator

For known variance Z test:
\(Z = \cfrac{(\bar{x} - \bar{y}) - (\mu_x - \mu_y)}{\sqrt{\cfrac{{\sigma}^2_x}{n_x} + \cfrac{{\sigma}^2_y}{n_y}}}\)

For unknown variance Z or T test:
\(Z = \cfrac{(\bar{x} - \bar{y}) - (\mu_x - \mu_y)}{\sqrt{\cfrac{\hat{\sigma}^2}{n_x} + \cfrac{\hat{\sigma}^2}{n_y}}}\)
or
\(T =\cfrac{(\bar{x} - \bar{y}) - (\mu_x - \mu_y)}{\sqrt{\cfrac{\hat{\sigma}^2}{n_x} + \cfrac{\hat{\sigma}^2}{n_y}}}\)

if it is a two tailed test, then obtain \(z_{\alpha/2}\) or
\(t_{\alpha/2 , n-2}\), else obtain \(z_{\alpha}\) or
\(t_{\alpha, n-2}\)(Note DGF = \(n-2\))

if \(|Z| < z\) or \(|T| < t\), \(H_0\) accepted, otherwise \(H_0\)
rejected

\paragraph{Conclusion}\label{header-n198}

Given the current data, there \{is/is not\} evidence to say that at
\(\alpha\)\% significant level, \{description of \(\mu_x,\,u_y\)
hypothesis\}

\paragraph{Confidence Interval}\label{header-n200}

For Z test with known variance:
\([(\bar{x} - \bar{y}) - z_{\alpha/2}\sqrt{\cfrac{{\sigma}^2_x}{n_x} + \cfrac{{\sigma}^2_y}{n_y}},(\bar{x} - \bar{y}) + z_{\alpha/2}\sqrt{\cfrac{{\sigma}^2_x}{n_x} + \cfrac{{\sigma}^2_y}{n_y}}]\)

For Z test with pooled variance:
\([(\bar{x} - \bar{y}) - z_{\alpha/2}\sqrt{\cfrac{\hat{\sigma}^2}{n_x} + \cfrac{\hat{\sigma}^2}{n_y}},(\bar{x} - \bar{y}) + z_{\alpha/2}\sqrt{\cfrac{\hat{\sigma}^2}{n_x} + \cfrac{\hat{\sigma}^2}{n_y}}]\)

For t test:
\([(\bar{x} - \bar{y}) - t_{\alpha/2,n-2}\sqrt{\cfrac{\hat{\sigma}^2}{n_x} + \cfrac{\hat{\sigma}^2}{n_y}},(\bar{x} - \bar{y}) + t_{\alpha/2,n-2}\sqrt{\cfrac{\hat{\sigma}^2}{n_x} + \cfrac{\hat{\sigma}^2}{n_y}}]\)

\subsubsection{Paired Test}\label{header-n204}

for a paired data \(X\) and \(Y\), you may conduct a test for the
difference \(D\) between \(X\) and \(Y\):

\begin{longtable}[]{@{}lllll@{}}
\toprule
\(X\) & \(x_1\) & \(x_2\) & \(\dots\) & \(x_i\)\tabularnewline
\midrule
\endhead
\(Y\) & \(y_1\) & \(y_2\) & \(\dots\) & \(y_i\)\tabularnewline
\(D = X-Y\) & \(x_1 - y_1\) & \(x_2 - y_2\) & \(\dots\) &
\(x_i - y_i\)\tabularnewline
\bottomrule
\end{longtable}

\paragraph{Define the variable{[}if needed{]}}\label{header-n225}

\paragraph{Hypothesis}\label{header-n226}

\(H_0\): \(\mu_D= \)a

\(H_1\): \(\mu_D \ne a, \mu_D > a, \mu_D < a\)

\paragraph{Test Statistics}\label{header-n229}

\(Z = \cfrac{\bar{D} - \mu_D}{\sqrt{\cfrac{\hat{\sigma}^2_D}{n_D}}}\) or
\(T = \cfrac{\bar{D} - \mu_D}{\sqrt{\cfrac{\hat{\sigma}^2_D}{n_D}}}\)

if it is a two tailed test, then obtain \(z_{\alpha/2}\) or
\(t_{\alpha/2 , n-1}\), else obtain \(z_{\alpha}\) or
\(t_{\alpha, n-1}\)(Note DGF = \(n - 1\))

if \(|Z| < z\) or \(|T| < t\), \(H_0\) accepted, otherwise \(H_0\)
rejected

\paragraph{Conclusion}\label{header-n233}

Given the current data, there \{is/is not\} evidence to say that at
\(\alpha\)\% significant level, \{description of \(\mu_D\) hypothesis\}

\paragraph{Confidence Interval}\label{header-n235}

For Z test:
\([\bar{D} - z_{\alpha/2}\sqrt{\cfrac{\hat{\sigma}^2_D}{n}},\bar{D} + z_{\alpha/2}\sqrt{\cfrac{\hat{\sigma}^2_D}{n}}]\)

For t test:
\([\bar{D} - t_{\alpha/2,n-1}\sqrt{\cfrac{\hat{\sigma}^2_D}{n}},\bar{D} + t_{\alpha/2,n-1}\sqrt{\cfrac{\hat{\sigma}^2_D}{n}}]\)

\subsection{Chi Square Test}\label{header-n238}

\subsubsection{Goodness of fit test}\label{header-n239}

\paragraph{Hypothesis}\label{header-n240}

\(H_0\): \{distribution\} is a good model to fit the data

\(H_1\): \{distribution\} is not a good model to fit the data

\paragraph{Frequency Table}\label{header-n243}

\begin{longtable}[]{@{}ll@{}}
\toprule
Number & Interval\tabularnewline
\midrule
\endhead
Observed & ...\tabularnewline
Expected & ...\tabularnewline
\(\cfrac{(O-E)^2}{E}\) & ...\tabularnewline
\bottomrule
\end{longtable}

\paragraph{Calculating Expected Value}\label{header-n257}

Expected Value can be calculated given the following situation:

\begin{itemize}
\item
  PDF \(f(x)\) given, \(N = N_{total} f(n)\) for each interval
\item
  Poisson distribution given, the first \(n-1\) term should be obtain
  from \(\cfrac{\lambda^x e^{-\lambda}}{x!}\), and the \(n^{th}\) term
  should be \(1-P(x<n)\)
\item
  Binomial distribution given, the term should be calculated as
  \(nCr * p^r (1-p)^{(n-r)}\)
\item
  Normal distribution given, the term should be \(N*P(n_1<x<n_2)\)
\end{itemize}

Note that before calculating the \(\chi\) score the expected value
should be combined to the interval next to it so that each interval for
expected value \textgreater{}5

\paragraph{Calculating the Test Statistics}\label{header-n269}

\(\chi ^2 = \sum\cfrac{(O-E)^2}{E}\)

\(DOF = N_{given \space outcome} - N_{parameters\space calculated} - 1\),
for example:

\begin{itemize}
\item
  for normal distribution if \(\bar{x}\) and \(\hat{\sigma_x}^2\)
  predicted from data, \(N_{parameters\space calculated} = 2\), and so
  on
\item
  for Poisson distribution if \(\lambda\) predicted from data,
  \(N_{parameters\space calculated} = 1\)
\item
  for Binomial distribution if \(p\) predicted from data,
  \(N_{parameters\space calculated} = 1\)
\end{itemize}

Obtain the \(\Chi^2_{DOF,\alpha }\) value from the MF10 booklet, if
\(\chi^2 > \Chi^2_{DOF,\alpha }\), then \(H_0\) rejected. Otherwise
\(H_0\) accepted

\paragraph{Conclusion}\label{header-n280}

Given the current evidence, there is \{enough/not enough\} evidence to
say that at \(\alpha %\) significant level the model\{distribution\} is
a good model for the data

\subsubsection{Test for Independent}\label{header-n282}

\paragraph{Hypothesis}\label{header-n283}

\(H_0\): \{variable I\} is independent of \{Variable II\}

\(H_1\): \{variable I\} is not independent of \{Variable II\}

\paragraph{Contingency Table}\label{header-n286}

For observed frequency, we would have the table here which the sum of
frequency we could obtained from the observed value:

\begin{longtable}[]{@{}llllll@{}}
\toprule
Observed Frequency & \(x_1\) & \(x_2\) & \(\dots\) & \(x_i\) &
Total\tabularnewline
\midrule
\endhead
\(y_1\) & & & & & \(\sum f_{i,1}\)\tabularnewline
\(y_2\) & & & & & \(\sum f_{i,1}\)\tabularnewline
\(\dots\) & & & & & \(\dots\)\tabularnewline
\(y_j\) & & & & & \(\sum f_{i,j}\)\tabularnewline
Total & \(\sum f_{1,j}\) & \(\sum f_{2,j}\) & \(\dots\) &
\(\sum f_{i,j}\) & \(\sum f\)\tabularnewline
\bottomrule
\end{longtable}

Hence we would be able to calculate the expected frequency from the
contingency table:

\begin{longtable}[]{@{}llllll@{}}
\toprule
Expected Frequency & \(x_1\) & \(x_2\) & \(\dots\) & \(x_i\) &
Total\tabularnewline
\midrule
\endhead
\(y_1\) &
\(f_{total} * \cfrac{f_{1,j}}{f_{total}}* \cfrac{f_{i,1}}{f_{total}}\) &
\(f_{total} * \cfrac{f_{2,j}}{f_{total}}* \cfrac{f_{i,1}}{f_{total}}\) &
\(\dots\) &
\(f_{total} * \cfrac{f_{i,j}}{f_{total}}* \cfrac{f_{i,1}}{f_{total}}\) &
\(\sum f_{i,1}\)\tabularnewline
\(y_2\) &
\(f_{total} * \cfrac{f_{1,j}}{f_{total}}* \cfrac{f_{i,2}}{f_{total}}\) &
\(f_{total} * \cfrac{f_{2,j}}{f_{total}}* \cfrac{f_{i,2}}{f_{total}}\) &
\(\dots\) &
\(f_{total} * \cfrac{f_{i,j}}{f_{total}}* \cfrac{f_{i,2}}{f_{total}}\) &
\(\sum f_{i,1}\)\tabularnewline
\(\dots\) & \(\dots\) & \(\dots\) & \(\dots\) & \(\dots\) &
\(\dots\)\tabularnewline
\(y_j\) &
\(f_{total} * \cfrac{f_{1,j}}{f_{total}}* \cfrac{f_{i,j}}{f_{total}}\) &
\(f_{total} * \cfrac{f_{2,j}}{f_{total}}* \cfrac{f_{i,j}}{f_{total}}\) &
\(\dots\) &
\(f_{total} * \cfrac{f_{i,j}}{f_{total}}* \cfrac{f_{i,j}}{f_{total}}\) &
\(\sum f_{i,j}\)\tabularnewline
Total & \(\sum f_{1,j}\) & \(\sum f_{2,j}\) & \(\dots\) &
\(\sum f_{i,j}\) & \(\sum f\)\tabularnewline
\bottomrule
\end{longtable}

\paragraph{Test Statistics}\label{header-n375}

\(\chi ^2 = \sum\cfrac{(f_o-f_e)^2}{f_e}\)

\(DOF = (row-1)(col-1)\)

Obtain the \(\Chi^2_{DOF,\alpha }\) value from the MF10 booklet, if
\(\chi^2 > \Chi^2_{DGF,\alpha }\), then \(H_0\) rejected. Otherwise
\(H_0\) accepted

\paragraph{Conclusion}\label{header-n379}

Given the current evidence, there is \{enough/not enough\} evidence to
say that at \(\alpha %\) significant level the two variables are
independent

Note that if \(H_0\) rejected, we could only say that the two variable
is \textbf{associated} but we cannot say they are causation

\subsection{Bivariate Data}\label{header-n382}

Given two variable y and x, if we attempt to find a correlation between
them, it is called regression. In FMA we are attempting to find the best
linear regression line between the two variable. This can be done via
the measure of least square, which is the sum of square of vertical
distance from the point to the regression line. This method is also
known as OLS(ordinary least square).

\href{}{Side note}: The reason we prefer OLS is that it is the best
unbiased estimator with least variance for the linear model

\subsubsection{Obtaining the regression line}\label{header-n385}

Given \(\hat{y} = a +bx\) as the regression line, we attempt to find
\((a,b)\) such that \(argmin_{(a,b)} \sum{[(b x_i + a)-y_i}]^2\) This
could be done by letting \(\cfrac{\partial E}{\partial a} = 0\) and
\(\cfrac{\partial E}{\partial b} = 0\)

Hence we obtained the expression for the coefficient for the regression
line:

\(b = \cfrac{S_{xy}}{S_{xx}} = \cfrac{\cfrac{\sum xy}{n} - \bar{x}\bar{y}}{\cfrac{\sum x^2}{n}-\bar{x}^2}\)
and \(a = \bar{y} - b\bar{x}\)

Notice that the regression line always past \(\bar{x}\) and \(\bar{y}\)

This is the coefficient for the regression line for y on x(taking x as
independent and y as dependent). Note during the exam you should plug in
the real value to the examiner to demonstrate you actually calculate
this

Similarly we can have regression line for x on y (taking y as
independent and x as dependent), which is \(\hat{x} = c + dy\)

the coefficient could be obtained similarly by using OLS:

\(d = \cfrac{S_{xy}}{S_{yy}} = \cfrac{\cfrac{\sum xy}{n} - \bar{x}\bar{y}}{\cfrac{\sum x^2}{n}-\bar{x}^2}\)
and \(c = \bar{x} - d\bar{y}\)

\subsubsection{Product moment correlation
coefficient}\label{header-n394}

The line of best fit can always be calculated, however the regression
line may or may not reflect the real linear relationship within the
data. Therefore we defined the measurement of PMCC to describe the
nature of the linear correlation.

\(r = \cfrac{S_{xy}}{S_x S_y} = \cfrac{\cfrac{\sum xy}{n} - \bar{x}\bar{y}}{\sqrt{\cfrac{\sum{x^2}-\bar{x}^2}{n}}\sqrt{\cfrac{\sum{y^2}-\bar{y}^2}{n}}}\)

which means that \(r^2 = bd\) as \(b\) and \(d\) are coefficient for
regression lines for y on x and for x on y

\begin{itemize}
\item
  b\textgreater{}0, \(r = \sqrt{bd}\) , the correlation is positive
\item
  b\textless{}0, \(r = -\sqrt{bd}\), the correlation is negative
\end{itemize}

\subsubsection{Comment on PMCC}\label{header-n403}

\(r \in [-1,1]\) and that:

Either

if \( 0.75 < |r| < 1\) then there is strong correlation

if \(|r| < 0.75\) then there is not strong correlation

Or:

if \(r > \rho_{\alpha\%, n}\) it is reliable

if \(r > \rho_{\alpha\%, n}\), it is not reliable

\subsubsection{Conducting Hypothesis Test on the
PMCC}\label{header-n411}

\paragraph{Hypothesis}\label{header-n412}

\begin{longtable}[]{@{}lll@{}}
\toprule
non-zero correlation & positive correlation & negative
correlation\tabularnewline
\midrule
\endhead
\(H_0\): \(\rho = 0\), \(H_1\): \(\rho \ne 0\) & \(H_0\): \(\rho = 0\),
\(H_1\): \(\rho > 0\) & \(H_0\): \(\rho = 0\), \(H_1\):
\(\rho < 0\)\tabularnewline
\bottomrule
\end{longtable}

The test for non-zero correlation is two tailed test, and the test for
positive and negative correlation is one-tailed test.

\paragraph{Test statistics}\label{header-n423}

The critical value for \(\rho_{n, sig \space level}\) can be obtained
from MF10, (notice the difference between two tailed and one tailed
significant level). If \(\rho < |r|\)1 reject \(H_0\), otherwise accept
\(H_0\),

\paragraph{Conclusion}\label{header-n425}

Given the current evidence, there is \{enough/not enough\} evidence to
say that at \(\alpha %\) significant level there is a
\{non-zero/positive/negative\} correlation between the two variable

\end{document}
